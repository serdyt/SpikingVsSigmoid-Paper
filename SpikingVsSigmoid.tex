\documentclass[conference, compsoc]{IEEEtran}

\usepackage{cite}


\usepackage{amsmath}
\usepackage{fixltx2e}
\usepackage{stfloats}
\usepackage{array}
\usepackage{tabularx}
\usepackage{pbox}
\usepackage{multirow}
\usepackage{rotating}



\begin{document}

\title{Computing with hardware neurons: spiking or classical?\\
	Perspectives of applied Spiking Neural Networks from the hardware side.}

\author{\IEEEauthorblockN{Sergei Dytckov}
\IEEEauthorblockA{University of Turku\\
	Finland}
\and
\IEEEauthorblockN{Masoud Daneshtalab}
\IEEEauthorblockA{Royal Institute of Technology\\
	Sweden}
}

\maketitle


\begin{abstract}
while classical neural networks take a position of a leading method in the machine learning community, spiking neuromorphic systems bring attention and large projects in neuroscience. Spiking neural networks were shown to be able to substitute networks of classical neurons in applied tasks. This work explores recent hardware designs focusing on perspective applications (like convolutional neural networks) for both neuron types from the energy efficiency side to analyse whether there is a possibility for spiking neuromorphic hardware to grow up for a wider use. Our comparison shows that spiking hardware is at least on the same level of energy efficiency or even higher than non-spiking on a level of basic operations. However, on a system level, spiking systems are outmatched and consume much more energy due to inefficient data representation with a long series of spikes. If spike-driven applications, minimizing an amount of spikes, are developed, spiking neural systems may reach the energy efficiency level of classical neural systems. However, in the near future, both type of neuromorphic systems may benefit from emerging memory technologies, minimizing the energy consumption of computation and memory for both neuron types. That would make infrastructure and data transfer energy dominant on the system level. We expect that spiking neurons have some benefits, which would allow achieving better energy results. Still the problem of an amount of spikes will still be the major bottleneck for spiking hardware systems.
\end{abstract}

\IEEEpeerreviewmaketitle



\section{Introduction}
Neural networks have gained quite a lot of attention last decades. From one side, there are large national and international projects such as Blue Brain, Human Brain project, the BRAIN initiative and a number of separate research groups bring up the advances in spiking neural networks (SNN). From another side, classical neurons with sigmoidal activation function show the possibility to outperform other machine learning algorithms. They are found to be very efficient for text, audio, and image recognition tasks \cite{Overview}. The Deep Neural Network (DNN) architectures as Convolutional Neural Networks (CNN) and Deep Belief Networks are extensively researched and already widely used in practice.

The main area of research on spiking neurons is in neuroscience (e.g. Brain modelling). Applied architectures for spiking neurons are much less explored, but show promising results. In simple cases, as the classification of the IRIS dataset \cite{Johnston2005} or prediction \cite{Johnson2009833}, SNN show accuracy comparable or even better than classical neurons. Adapting CNN to spiking neurons shows slightly worse accuracy on complex image recognition tasks \cite{SNN_DCNN_object_rec,threshold_balancing}.

Low energy consumption is widely believed to be an inherent feature of SNN. Spiking neuron, implemented using physical characteristics of transistors working in subthreshold regime, show the promising energy result of a few $ pJ $ per spike \cite{Indiveri,Phong,HardwareOrAnalog,Wijekoon_compact_silicon_circuit,Energy-Efficient-Neuron}, potentially making neuromorphic hardware (VLSI of silicon spiking neurons) suitable for an energy efficient computing of applications, where accuracy is not critical. When a new platform is introduced, a comparison is typically made with either a general purpose CPU or previous works on the same type of neuron. To our knowledge, there are only two works have compared spiking and non-spiking systems. Another work compares spiking and non-spiking digital VLSI ANN implementations with an application to MNIST database classification \cite{Temam:comparison}. But the reported values are controversial. For energy comparison, 500 cycles of SNN are considered, resulting in more than x3000 times more energy consumption than classical ANN. That is equal to x7 larger energy consumption per cycle for SNN. Although another test on a small dummy network shows that SNN is x2 times more energy efficient per cycle. Despite this, the conclusion is that SNN are not competitive to classical ANN due to a large simulation time although SNN cost twice smaller area. FPGA implementations are explored in \cite{Johnston2005}. But only area (FPGA slice) utilisation is considered, which is slightly lower for the simple spiking neuron model. In this paper, we explore the hardware realization of spiking neurons and networks from both digital and analog implementation and compare them with classical neural systems. The key objective is in the energy comparison, while exploring the possibility of neuromorphic hardware to be used in practical applications.

This paper is built in the order of growing complexity. Section 2 provides a necessary background on neural networks and the neuron models. Section 3 explains the methods and metrics we used for comparison of spiking and non-spiking neural hardware. Section 4 gives a comparison of individual neurons and their individual operations. Section 5 compares hardware accelerators for ANN on the system level. Section 6 explores an effect of emerging resistive memory technology on the implementation of ANN hardware.

\section{Background on neural networks}
On the level of individual processing blocks, a classical neuron receives $ N $ pairs of inputs $ x $ and weights $ w $ and performs $ N $ multiplications on them, then the weighted inputs are summed with $ N-1 $ additions and send to the sigmoidal-like kernel, equations (1,2). The whole operation can be easily pipelined, time multiplexed and partially computed. Sigmoidal kernel is typically implemented as a LookUp Table (LUT).
\begin{equation} \label{eq:1}
y=F(\sum_{i}^{N}\omega_{i}x_{i})
\end{equation}
\begin{equation} \label{eq:2}
F(t)=(\frac{1}{1+e^{i}})
\end{equation}

Spiking neurons are very similar to classical neurons from the architectural point of view. A neuron has a fixed amount of weighted inputs - synapses, and the kernel performs some operations on them. The activation function is replaced with a more complex model, which simulates the behavior of neuron’s membrane potential in time and its reaction to input - spikes. Spiking neurons receive a distributed in time sequence of spikes. Spikes are essentially boolean values, so the weights are injected (accumulated) directly into the neuron kernel. The computational complexity highly depends on the model. Here we concern the simplest Leaky Integrate and Fire model (LIF) \cite{Gerstner:SNN_models}, see equation (3). This model describes the membrane potential $ V $, which receives input $ I $ and constantly leaks toward resting potential $ E $ with the speed controlled by $ \tau $.When the threshold potential $ V_{thr} $ reached, the membrane potential is reset to $ V_{reset} $ and neuron emits an output spike. The LIF model is able to fire only a regular output spiking rate in response on a constant input, which is not enough for biological simulations, but it is fine for practical computations. In the simplest case, input $ I $ is just a pulse corresponding to a weight of an input spike, however synapse may have a more complex model, which provides input distributed in time, so called “current based synapse”.
\begin{equation} \label{eq:3}
\begin{split}
&\frac{dV}{dt}=I+(V(t)-E)/\tau\\
& if~V=V_{thr}\Rightarrow V=V_{reset}
\end{split}
\end{equation}

In this paper, we consider ANN as a machine learning method, implementing a practical task. Training of an ANN consists of adjusting synaptic weights. It is typically a very long iterative process. The best known learning rules are error backpropagation for classical neurons \cite{Backpropagation} and spike-timing dependent plasticity (STDP) for spiking neurons \cite{STDP}. To make it clear, we do not consider the learning process in this paper.

Implementation of both spiking and classical neurons is explored in both digital and analog domains. For classical neurons, the benefit of analog circuits is significant \cite{Esmaeilzadeh:Approximate,Esmaeilzadeh:Approximate_analog}, but we were unable to find any recent works with the breakdown data of individual components to include in this paper. The behavior of a spiking neuron is modeled efficiently by a few transistors working in the deep subthreshold regime \cite{Indiveri,Phong,HardwareOrAnalog,Wijekoon_compact_silicon_circuit,Energy-Efficient-Neuron}, plus it inherently supports  the continuous asynchronous simulation. Digital spiking neurons suffer from a complex model, which requires continuous recalculation even when there is no input spike. This becomes worse when the input data should be encoded by spike-rate which increases simulation time significantly. To address this issue, over-simplified models are commonly used. 

Large classical ANN may be easily simulated on a limited number of time-multiplexed neurons (either digital or analog) as there is no state in classical neurons. Digital spiking neurons may be time-multiplexed, but it is costlier, as neuron state (membrane potential $ V $ in the simplest case of equation (3)) need to be saved. Analog spiking neurons, theoretically, may also be time-multiplexed, if there are no cyclic connections in a network, but, to our knowledge, this has not been explored in the literature. Thus, most of the digital hardware designs for both neuron designs exploit the time-multiplexing feature to minimise area consumption, while analog spiking hardware is typically implemented in a full parallel way.


\section{Methods of comparison}
To have a fair comparison, we considered two granularity levels: a) the level of individual neurons and b) the whole system level. The key assumptions to make a comparison possible are as following: (1) one input of a classical neuron is equivalent to one synaptic connection of a spiking neuron and (2) the structure of neural networks (the number of neurons and their connections) for both classical and spike neuron types are similar for the same applications.

\textit{Energy per spike} is measured as a key metric for analyzing analog neurons, however this metric depends on a benchmark. For spiking neurons, most commonly, the energy consumption is recorded by applying a strong constant input, which charges the capacitor above the threshold of a neuron. This benchmark is not representative, as in practice a neuron integrates multiple input spikes, distributed in a significant time window, to produce an output spike. Thus, the total amount of input, integrated into the neuron capacitor, commonly exceeds a threshold several times, due to the leakage component. Current injection infrastructure, as digital-to-analog converter (DAC), needed to convert synaptic weighted into input current, is rarely included in a benchmark, making the estimations too optimistic. We find that the \textit{energy per synaptic event} (a single input spike coming to a single neuron) is more representative and realistic metric. It is rarely used on the level of individual neurons, but may be extracted from the full system designs.

For classical neurons, power and timing characteristics are commonly available. Out of that, we estimate energy consumption per operation. In fact, we calculate the energy consumption per single input to classical neurons and compare it with a single input or output spike.

For the full system comparison, in most cases, we also estimate energy consumption using power values and timing. We try to scale the energy values for a single benchmark, which is the edge detection on a small 18x18 image with 3x3 kernel size. That is the first and the main comparison metric in this paper. Edge detection is a simple CNN, consisting of a single convolutional layer of 4 directional convolutions and one max, sum or average pooling layer. Classical neurons are able to represent both positive and negative outputs, thus only 2 convolutional maps, with an absolute function, may be used. 

In some cases, \cite{NeuFlow,DVS_analog,DVS_digital,CAVIAR}, it is hard to scale the reported energy values to get the first metric due to a complex benchmark used in the original works. Thus, we take the second metric, which is the number of \textit{giga-operations per second per watt} ($ GOPS/W $) for classical neurons or \textit{giga-synaptic operation per second per watt} (GSOPS/W) for spiking neurons. With the assumption (1), we may directly compare $ GOPS/W $ with $ GSOPS/W $. This metric shows the system energy efficiency level, however, it does not take into account the SNN application specific that more than one spike is used for a single operation.

The largest part of the systems considered in this work are oriented towards CNN as the state of the art ANN architecture. The results, presented in Sections 4 and 5, should be taken with a grain of salt, as a simple linear scaling applied in this work is not accurate. For example, in Origami system \cite{Origami}, reported energy and $ GOPS/W $ were obtained from the configuration of 7x7 kernel size with 4 hardware neurons implementing 8 convolutional kernels in parallel. The values were scaled down x22 times to represent a convolution window of 3x3 pixels with just 2 kernels.

\section{Silicon neurons}
Table \ref{table:1} combines the data for the classical neuron cores. Values for individual synapses (basically multiplier, adder and an insignificant fraction of an activation function) are calculated from the system level data. Energy values are not reported directly in the original published works, but we have extracted them from some other reported data (like power and timing). These extracted data are highlighted with an underline in all tables throughout the paper.

In \cite{DianNao}, a neuron with 16x 16-bit inputs was implemented in a pipeline manner. Parallel multipliers in the first layer, adder tree in the second, and finally sigmoidal kernel implemented as a piecewise linear approximation with LUT. In the original paper a power of 16 neurons is reported, thus we are able to extract the energy for a single neuron, which is 0.5$ pJ $ per input. This value should be taken accurately, as the system was benchmarked with full CNN applications, most of which had a small convolutional kernel and includes pooling layers (e.g. average operation on a 2x2 kernel). Thus, the neurons were highly underutilised and consumes less power. In \cite{Temam:defect_tolerant}, a defect tolerance of hardware neurons is explored. A single layer of 10 neurons, each with 90x 16-bit inputs, is implemented. Dividing the total energy by the number of synapses, we calculate the energy consumption per synapse to be 78$ pJ $. For the last row of Table 1, we implemented a single neuron with 16-bit fixed point input and synthesized it by Cadence RTL tool with the 65$ nm $ process.
\begin{table}[h]
	\caption{classical digital neuron core costs and efficiency}
	\label{table:1}
	\centering
	\setlength{\tabcolsep}{5pt}
	\begin{tabular} {| c | c | c | c | c |}
		\hline Ref. & \pbox[t]{40pt}{Tech. node,\\ $ nm $} & Area, $ \mu m^{2} $  & Power, $ W $ & \pbox[t]{45pt}{Energy$ / $syn.,\\ $ pJ $} \\ 
		\hline \cite{DianNao} & 65 & \underline{52903} & \multirow{4}{10pt}  & \underline{0.5} \\ 
		\hline \cite{Temam:defect_tolerant} & 90 &  &  & \underline{78} \\ 
		\hline Our exp. & 65 & 18643 & 7.2e-03 & \underline{8} \\ 
		\hline 
	\end{tabular}
\end{table}

Data on spiking neurons are quite fragmented. We have collected the data from different papers in Table \ref{table:2}. Most of the digital designs have been targeted field-programmable gate array (FPGA). As the power value of FPGA is only reported for the whole chip, no matter how much of it is utilised, their reports were not useful for us. A review of the large amount of models, implemented as analog circuits, has been presented in \cite{Indiveri}. Most of the models there are biologically plausible, which makes them energy and area costly. We focus on simple models, which show better area and energy results, and potential for practical applications.

In \cite{Phong}, different modulation methods with current injecting into membrane capacitor are presented. The most efficient is the current modulation, which consumes 0.25$ pJ $ with 7-bit current precision. This energy value is recorded when a maximal input, higher than the threshold, is injected. The large area of the neuron is driven by the need for a large DAC to implement multiple current levels. The energy consumption in the realistic case is likely to be larger due to a clocked latch comparator, which dissipates a significant portion of the neuron power in case of continuous simulation.

In \cite{HardwareOrAnalog}, both analog and digital designs are compared at 65$ nm $ process. In both cases, an input spike is injected with the time-based method as following. In the digital design, the  input is integrated into a counter based on a number of cycles, similarly, in the analog design, the number of equal current pulses is integrated into a capacitor. The analog design consumes 2$ pJ $, while digital design 41$ pJ $. That is the difference in an order of magnitude. Another result for a digital neuron is obtained from the system level values of TrueNorth architecture \cite{TrueNorth}. See the details of the architecture in Section 5. The reported energy consumption is 26$ pJ $ per synaptic event. This value, however highly depends on the network connectivity and activity.

In \cite{Wijekoon_compact_silicon_circuit}, a model capable of a wide variety of biologically plausible responses is implemented as an analog circuit. A strong input voltage causing 100$ Hz $ output spike frequency results in 8.5-9$ pJ $ energy consumption per output spike at 0.35 μm process. However, no spike injection circuitry is presented in that work. In \cite{Energy-Efficient-Neuron}, a neuron together with a synapse, implementing STDP rule, were implemented at 90$ nm $ process. A neuron core was studied separately with an external voltage source, i.e. without any injection circuitry. The area for the neuron core and synapse is 442 $ \mu m^{2} $ and 4823 $ \mu m^{2} $ respectively, while each synapse in this model should be physically implemented as it contains a weight value internally in a capacitor. The resulting energy consumption of the neuron core  is just 0.4$ pJ $. The plasticity rule is activated on both input and output spikes and consumes 0.37$ pJ $ per each spike, but in this case energy dissipation for charging the neuron cell capacitor is omitted. We ignore plasticity rules in this work, but, we include it in our table due to the energy efficiency level of the neuron. In \cite{Subthreshold_Izhikevich}, a model with biologically realistic spike behavior was implemented in silicon. It occupies a significantly larger area than other models, even though the 90$ nm $ technology was utilised. The reported energy consumption per spike is 1$ pJ $, and the power consumption in the resting state is reported being 35 times less than in the spiking mode. Current injection circuitry is omitted.

In \cite{DVS_analog,CAVIAR}, a chip for spiking convolution is implemented, see the details of the architecture in Section 5. There are two unique features of this implementation. The first is the presence of positive and negative spike types and two comparators to detect two thresholds. The second feature is the calibration circuits to address hardware mismatch. This requires 2930 $ \mu m^{2} $ area at 0.35$ \mu m $ technology. The energy consumption on the chip level highly depends on the convolutional kernel size. To minimize the overhead of the infrastructure (I/O pads, memory), we assume the situation with the highest available kernel size when each input spike is addressed to every neuron. The energy consumption in this case is 4$ pJ $ per synaptic event.
\begin{table}[h]
	\caption{spiking neuron core cost and efficiency}
	\label{table:2}
	\centering
	\setlength{\tabcolsep}{5pt}
	\begin{tabular} {|c|c|c|c|c|c|}
		\hline Ref. &  \pbox[t]{40pt}{Tech. node,\\ $ nm $} & & Area, $ \mu m^{2} $  & \pbox[t]{35pt}{Energy/\\spike, $ pJ $} & \pbox[t]{30pt}{Energy/\\syn., $ pJ $}\\ 
		\hline \cite{Phong} & 65 & \multirow{6}{*}{\begin{turn}{90}Analog\end{turn}} & 5000 & 0.25 & \\
		\cline{1-2}\cline{4-6} \cite{HardwareOrAnalog} & 65 &  & 1300 & 2 & \\
		\cline{1-2}\cline{4-6} \cite{Wijekoon_compact_silicon_circuit} & 350 &  & 1600 & 8.5-9 & \\
		\cline{1-2}\cline{4-6} \cite{Energy-Efficient-Neuron} & 90 &  & 442 / 4823 & 0.4 & 0.37 \\
		\cline{1-2}\cline{4-6} \cite{Subthreshold_Izhikevich} & 90 &  & 2980 & 1 &  \\
		\cline{1-2}\cline{4-6} \cite{CAVIAR}\cite{DVS_analog} & 350 &  & \underline{2930} & & 4 \\
		\hline \cite{HardwareOrAnalog} & 65 & \multirow{2}{*}{\begin{turn}{90}Digi.\end{turn}} & 538 & 43 & \\	[0.25pt]
		\cline{1-2}\cline{4-6} \cite{TrueNorth} & 28 &  &  &  & 26 \\	[0.25pt]
		\hline
	\end{tabular} 
\end{table}

\subsection{Discussion}
The first thing to note from the results is that digital spiking neurons are not energy efficient compared with the analog alternative, despite having the benefit of technology scaling. Analog spiking designs, in turn, have about the same or better energy efficiency than classical neurons. However, in the majority of applications, the spike rate encoding is used, which requires to process a large spike train to extract an output. This mitigates the advantage of spiking neurons. Moreover, the values in Table 2 are optimistic due omitting injection circuits in most of the silicon spiking designs.

Although values in Table 1 and 2 have a large variance and there are not enough data on classical neurons, classical digital neurons consume in a range of tens of $ pJ $ per one synapse activation while spiking neurons consume in range of single $ pJ $. That would be our initial point of view on the problem: analog spiking neurons consume about an order of magnitude lower energy per single operation (synapse / spike). 

\section{Silicon neural network system}
Table 3 has an overview of full system designs for both classical and spiking neurons. We focus on CNN as the state of the art ANN architecture. We select the hardware architectures reporting the energy values possible to scale towards the same benchmark - the edge detection on 18x18 image with 3x3 convolution kernel and 4 directional maps. To compare the energy efficiency of systems with different types of neuron, we introduce the energy column, by estimating the time required to process 18x18 image and multiplying it by reported power value or scaling the reported energy results where available. All the results extracted from values reported in the original papers are highlighted with the underline.

Due to limited available materials in hardware realization of spike-based systems, in order to complement the results, we have designed our baseline spiking neural platform and performed the simulation with the selected benchmark to extract the energy values. We used the analog circuit of spiking neuron \cite{Phong}, as the power of separate components of the neuron is reported. Neurons are placed into a cluster (56 neurons in a cluster) and connected through a regular 2D mesh with a packet-switch dimension-order routing. Each cluster has a limited number of input “axons”. Spikes are delivered as a unicast packets through the network based on a destination address and multicasted inside the cluster based on the local axon ID. The design is modeled with systemC (based on Noxim simulator \cite{NOXIM}) and the energy consumption is calculated based on the system activity as network traffic, number of memory accesses, number of spikes generated... The energy values of all the cluster components in each cluster have been back annotated from the synthesized results extracted with Cadence RTL tool, network on chip energy have been back annotated with the help of Orion simulator \cite{Orion}.

\begin{table}[h]
	\caption{full system comparison}
	\label{table:3}
	\centering
	\setlength{\tabcolsep}{5pt}
	\begin{tabular} {|c|c|c|c|c|c|}
		\hline Ref. &  \pbox[t]{40pt}{Tech. node,\\ $ nm $} & & & \pbox[t]{40pt}{Benchmark\\energy, $ J $} & GOPS/W \\ 
		\hline \cite{DianNao} & 65 & \multirow{4}{*}{\begin{turn}{90}Classical\end{turn}} & \multirow{4}{*}{\begin{turn}{90}Digital\end{turn}} & \underline{1e-08} & 931 \\
		\cline{1-2}\cline{5-6} \cite{HWCE} & 28 &  &  & \underline{1e-08} & 412 \\
		\cline{1-2}\cline{5-6} \cite{NeuFlow} & 45 &  &  &  & 230 \\
		\cline{1-2}\cline{5-6} \cite{Origami} & 65 &  &  & \underline{2e-08} & 369 \\
		\hline \cite{12p} & 65 & \multirow{5}{*}{\begin{turn}{90}Spiking\end{turn}} & \multirow{3}{*}{\begin{turn}{90}Analog\end{turn}} & 1e-07 -- 3e-06 &  \\
		\cline{1-2}\cline{5-6} Our exp. & 65 &  &  & 1e-07 -- 5e-06 &  \\
		\cline{1-2}\cline{5-6} \cite{CAVIAR}\cite{DVS_analog} & 350 &  &  &  & \underline{1.4--225} \\	
		\cline{1-2}\cline{4-6} \cite{DVS_digital} & 350 &  & \multirow{2}{*}{\begin{turn}{90}Digi.\end{turn}} &  & \underline{2.6-7.5} \\
		\cline{1-2}\cline{5-6} \cite{TrueNorth} & 28 &  &  & \underline{5e-06} & 46-400 \\				
		\hline
	\end{tabular} 
\end{table}

\subsection{Discussion}
From Table 3, we can see that the energy consumption on the selected benchmark for systems using non-spiking neurons is on the level of tens of nanojoules. The energy for spike-based systems is larger for about 1-2 orders of magnitude, despite the fact that spiking neurons individually are more energy efficient , see Section 4. This is mainly due to the use of spike-rate encoding, i.e. to encode an 8-bit number a large spike sequence is required. Together with that, power hungry components such as current injection circuits, weight storage and interconnection network also play the dominant roles in the system energy. In our experiment, described in Section 5.2, the impact on the \textit{energy per spike} of infrastructure for neurons (interconnection network, memory storage, injection circuits) imposes almost two orders of magnitude energy overhead, from 0.25$ pJ $ in \cite{Phong} to 13$ pJ $ in our experiment. The results in \cite{12p}, where the interconnection network is simplified almost to a level of direct connections, 98\% of total energy is consumed by memory accesses. In classical systems, infrastructure also consumes the most energy although the impact of the neuron itself is also significant, e.g. in the DianNao architecture \cite{DianNao}, the arithmetic block consumes about 27\% of the total energy.

It is hard to extract energy values from neuromorphic systems \cite{DVS_analog,CAVIAR,DVS_digital} as the power dissipation is reported on the chip level and benchmark is not clearly defined, so that we use the second available metric - $ GOPS/W $. In the Caviar system’s convolutional chip \cite{DVS_analog,CAVIAR}, the kernel size may vary in the range from a single destination up to 32x32 window. When each packet with a spike coming to the chip is delivered to a single neuron, all the I/O costs is paid for a single activation. This corresponds to a low $ GOPS/W $ value. When each spike is delivered to multiple destination neurons the infrastructure energy consumption is amortised by a large amount of synaptic operations. The higher value of 225 GOPS/W in Table 3 for \cite{DVS_analog,CAVIAR} corresponds to the maximum kernel size. We consider this high value for comparison as it represents the situation when hardware resources are optimised for a specific task, as it is done in \cite{HWCE,Origami}, where the amount of multipliers corresponds to the size of convolutional window. Digital systems approach \cite{DVS_digital} shows much worse energy efficiency of 7.5$ GOPS/W $, which only slightly depends on kernel size. This shows that the energy consumption of digital neuron simulation is comparable to infrastructure energy. The TrueNorth architecture \cite{TrueNorth} also reports a high energy efficiency up to 400$ GOPS/W $ for a high-connection network. Classical neural systems report an energy efficiency from 230 to 930$ GOPS/W $, which is achieved by high data reuse and data transfer minimisation as a sequence. These values show that both spiking and non-spiking systems are on a similar level of energy efficiency \textit{per synapse}.

The main conclusion is that the hardware efficiency of neuromorphic platforms is about the same as for classical neuron systems, in terms of single operations. However, the single operator (spike) is typically not enough to represent data, as spike-frequency encoding is commonly used, resulting in significantly higher energy consumption. Thus, a primary milestone to enable neuromorphic computing for applied applications would be the development of applications implementation with alternative (time-based) data encoding requiring less number of spikes. This would allow breaking the distance between spiking and classical systems. Such techniques are known for quite a long time \cite{Thorpe:encodings}, but they are rarely utilised.

From another side, even if we assume that spiking applications with single spike data encoding would be developed, resulting in a similar energy consumption of spiking and classical ANN, the area overhead of classical systems is much lower due to time-multiplexing designs, whereas spiking systems have to be implemented in parallel. Despite the size of a silicon spiking neuron measured in thousands of square micrometers, millions of neurons are required to perform, for example, an image recognition with an acceptable resolution. Thus, we do not expect spiking neural systems to be able to compete with classical approach in computational applications.

\section{Memristor crossbar based systems}
The emergence of resistive memory cells opens up a new horizon in energy efficiency for ANN. Memristors, combined in a crossbar fashion - memristor crossbar (MC), replace SRAM memory for weight storage and implement a vector-matrix multiplication of input voltages to memristor conductances. That is beneficial for spiking neurons, where SRAM access energy is dominant (98\% in \cite{12p}), and for classical neurons, where both memory and synaptic multiplications are important (66\% in \cite{DianNao} Together with energy, MC is highly dense and scalable, which is important for densely connected ANN, where synapses occupy a huge area in VLSI circuits.

Sneak paths are a problem with a pure MC, as current may go through any path of it, even though we want to access a specific cell. Classical ANN allows mitigating this, as all the inputs are injected simultaneously during the read operation \cite{Memristor_approximated,RRAM_approximated}. SNN do not have this feature and thus need some workaround to address the problem of sneak paths, as well as memristor programming requires an access to individual elements. Some works utilise 1T1M (one transistor per memristor) methodology, which is used to access individual memristors and remove the sneak path problem \cite{Spiking_resistive_crossbar,STT_SNN,ex_situ}. Another method of mitigating sneak paths during write operation is to apply half of programming voltage to all the rows and columns except of a selected one, which receives a whole programming voltage on its row and zero voltage on its column \cite{RRAM_approximated,Segmented_memristor}.

A property of memristors to tune resistance gradually, when small programming pulses are applied, is very well popular in neuromorphic community. It allows emulating STDP learning rule right in the neurons without much extra circuitry. In this paper, we consider only a forward path of ANN, assuming that ANN is pre-trained offline. This is an ideal scenario, yet it is quite realistic, for example in sensor nodes ANN should  execute a pre-processing function on input data. Thus we have not considered  those works using learning rules such as back propagation for classical and STDP for spiking neurons.

MC, implementing classical ANN, makes an idea of neural approximated computing very attractive (when a tightly coupled neural accelerator approximates parts of an application, see \cite{Esmaeilzadeh:Approximate,Esmaeilzadeh:Approximate_analog}). This idea is explored in \cite{Memristor_approximated,RRAM_approximated}, where a group of memristive neurons implement two layered ANN serving as universal approximator provides up to 568$ GOPS/W $ energy efficiency. In this case, one approximated operation (CPU instruction) is used as a metric, rather than one synaptic MAC operation in Table 4. In \cite{Memristor_approximated}, 270x energy efficiency improvement over Intel Xeon processor and 12x improvement over FPGA for calculating Gaussian distance (part of HMAX benchmark) are reported.
\begin{table}[h]
	\caption{memristor crossbar based architectures}
	\label{table:4}
	\centering
	\setlength{\tabcolsep}{5pt}
	\setlength\extrarowheight{1pt}
	\begin{tabular} {|c|c|c|c|}
		\hline Ref. &  \pbox[t]{40pt}{Tech. node,\\ $ nm $} & & \pbox[t]{45pt}{Energy$ / $syn.,\\ $ pJ $}\\ 
		\hline \cite{STT_SNN} & 45 & \multirow{2}{*}{\begin{turn}{90}Class.\end{turn}} & \underline{1.4} \\
		\cline{1-2}\cline{4-4} \cite{Segmented_memristor} & 45 &  & 1.8--21 \\
		\hline \cite{Can_we_use_SNN_RRAM} &  & \multirow{3}{*}{\begin{turn}{90}Spiking\end{turn}} & \underline{0.5} \\
		\cline{1-2}\cline{4-4} \cite{Spiking_resistive_crossbar} & 130 &  & \underline{12} \\
		\cline{1-2}\cline{4-4} \cite{Rajendran:specification_of_nanosscale} & 10 &  & 41 \\
		\hline
	\end{tabular} 
\end{table}

\subsection{Discussion}
Emerging resistive memory technology benefits both type of neurons. MC makes memristor access as the basic synaptic operation. This should results in the energy consumption of a few $ fJ $ for both neuron types, as shown in Table 4. Although, on this early stage of memristive technologies, the variance in the reported data is very high. With such a low value for synapses, the total system energy consumption would be dominated by other parameters such as data transmission, ADC/DAC (if we assume that communication, especially in large-scale systems is digital). Spike based systems may have some benefits in that. First, as a spike is an event, a minimal network payload is generated, whereas classical neurons exchange N-bit values. Second, data input to MC requires just a single input voltage level, while classical neurons would require multiple voltage levels and more complex DAC, as well as ADC, to read an output. Such, in \cite{Merging_Interface} ADC/DAC is estimated to consume more than 85\% of total power and area for 8-bit accuracy. Authors propose to expand MC and use separate rows and columns for each input and output bit, which saves, by their estimations, 60--85\% of total power and 50--85\% of total area (as MC is very compact). Although another work \cite{STT_SNN} reports less than 25\% of total energy consumption for current source. Third, there are some potential applications (like DVS based surveillance), where input is essentially spiking and don’t need a conversion into spikes. DVS, also, typically require a small amount of spikes to represent the data. For example, in \cite{DVS_analog}, sensor registering the silhouettes of two walking people on 128x128 resolution produce 980 spikes at 40 ms, a normal 25 fps camera would produce 16384 input pixels. Thus the amount of data to process is lower for an order of magnitude for DVS. Together with spike-time based processing it would give an order of magnitude of energy efficiency.

\section{Conclusion}
With currently available technologies, spiking neuromorphic systems lose classical non-spiking systems on every position (accuracy, area, energy, speed). There is an essential disadvantage in spike-rate encoding, which requires multiple spikes to be processed. Having applications with single-spike data representation may bring spike based systems in-line with classical in the energy efficiency. Memristive memory technology gives a huge benefit for both neuron types, replacing the synaptic circuits. With that, synapse and neuron access become an insignificant part of the system energy consumption. Architecturally, classical ANN accelerators would become very similar to spiking designs. With the similar energy consumption in the synapses for both neuron types, other parameters would become the energy bottleneck. We expect that spiking neurons may have some benefits on a system level, which could bring better energy results. Although there is still the need to mitigate an application level inefficiency of spike-data encoding for SNN.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,SpikingVsSigmoid}{}



% that's all folks
\end{document}


